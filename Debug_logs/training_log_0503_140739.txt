INFO 05-03 14:07:48 [__init__.py:239] Automatically detected platform cuda.
2025-05-03 14:07:55,522	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(pid=1212922)[0m INFO 05-03 14:08:01 [__init__.py:239] Automatically detected platform cuda.
[36m(Runner pid=1212922)[0m {
[36m(Runner pid=1212922)[0m   "data": {
[36m(Runner pid=1212922)[0m     "train_files": "BLINK-Benchmark/BLINK",
[36m(Runner pid=1212922)[0m     "val_files": "BLINK-Benchmark/BLINK",
[36m(Runner pid=1212922)[0m     "prompt_key": "problem",
[36m(Runner pid=1212922)[0m     "answer_key": "answer",
[36m(Runner pid=1212922)[0m     "image_key": "images",
[36m(Runner pid=1212922)[0m     "max_prompt_length": 2048,
[36m(Runner pid=1212922)[0m     "max_response_length": 2048,
[36m(Runner pid=1212922)[0m     "rollout_batch_size": 16,
[36m(Runner pid=1212922)[0m     "val_batch_size": -1,
[36m(Runner pid=1212922)[0m     "format_prompt": "/home/stud/wxie/EasyR1/examples/format_prompt/tools_thinker_format.jinja",
[36m(Runner pid=1212922)[0m     "shuffle": true,
[36m(Runner pid=1212922)[0m     "seed": 1,
[36m(Runner pid=1212922)[0m     "max_pixels": 4194304,
[36m(Runner pid=1212922)[0m     "min_pixels": 262144,
[36m(Runner pid=1212922)[0m     "filter_overlong_prompts": false,
[36m(Runner pid=1212922)[0m     "subtasks": [
[36m(Runner pid=1212922)[0m       "Counting"
[36m(Runner pid=1212922)[0m     ],
[36m(Runner pid=1212922)[0m     "dataset_prefix": "/home/stud/wxie",
[36m(Runner pid=1212922)[0m     "tools_config": "./examples/tools_config/tools_configuration_file.yaml"
[36m(Runner pid=1212922)[0m   },
[36m(Runner pid=1212922)[0m   "worker": {
[36m(Runner pid=1212922)[0m     "hybrid_engine": true,
[36m(Runner pid=1212922)[0m     "actor": {
[36m(Runner pid=1212922)[0m       "strategy": "fsdp",
[36m(Runner pid=1212922)[0m       "global_batch_size": 8,
[36m(Runner pid=1212922)[0m       "micro_batch_size_per_device_for_update": 4,
[36m(Runner pid=1212922)[0m       "micro_batch_size_per_device_for_experience": 8,
[36m(Runner pid=1212922)[0m       "max_grad_norm": 1.0,
[36m(Runner pid=1212922)[0m       "clip_ratio_low": 0.2,
[36m(Runner pid=1212922)[0m       "clip_ratio_high": 0.3,
[36m(Runner pid=1212922)[0m       "clip_ratio_dual": 3.0,
[36m(Runner pid=1212922)[0m       "ppo_epochs": 1,
[36m(Runner pid=1212922)[0m       "padding_free": true,
[36m(Runner pid=1212922)[0m       "ulysses_sequence_parallel_size": 1,
[36m(Runner pid=1212922)[0m       "use_torch_compile": true,
[36m(Runner pid=1212922)[0m       "model": {
[36m(Runner pid=1212922)[0m         "model_path": "Qwen/Qwen2.5-VL-3B-Instruct",
[36m(Runner pid=1212922)[0m         "tokenizer_path": "Qwen/Qwen2.5-VL-3B-Instruct",
[36m(Runner pid=1212922)[0m         "override_config": {},
[36m(Runner pid=1212922)[0m         "enable_gradient_checkpointing": true,
[36m(Runner pid=1212922)[0m         "trust_remote_code": false,
[36m(Runner pid=1212922)[0m         "freeze_vision_tower": false
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "optim": {
[36m(Runner pid=1212922)[0m         "lr": 1e-06,
[36m(Runner pid=1212922)[0m         "betas": [
[36m(Runner pid=1212922)[0m           0.9,
[36m(Runner pid=1212922)[0m           0.999
[36m(Runner pid=1212922)[0m         ],
[36m(Runner pid=1212922)[0m         "weight_decay": 0.01,
[36m(Runner pid=1212922)[0m         "strategy": "adamw",
[36m(Runner pid=1212922)[0m         "lr_warmup_ratio": 0.0,
[36m(Runner pid=1212922)[0m         "min_lr_ratio": null,
[36m(Runner pid=1212922)[0m         "warmup_style": "constant",
[36m(Runner pid=1212922)[0m         "training_steps": -1
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "fsdp": {
[36m(Runner pid=1212922)[0m         "enable_full_shard": true,
[36m(Runner pid=1212922)[0m         "enable_cpu_offload": false,
[36m(Runner pid=1212922)[0m         "enable_rank0_init": true,
[36m(Runner pid=1212922)[0m         "use_orig_params": false,
[36m(Runner pid=1212922)[0m         "torch_dtype": "bf16",
[36m(Runner pid=1212922)[0m         "fsdp_size": -1,
[36m(Runner pid=1212922)[0m         "mp_param_dtype": "bf16",
[36m(Runner pid=1212922)[0m         "mp_reduce_dtype": "fp32",
[36m(Runner pid=1212922)[0m         "mp_buffer_dtype": "fp32"
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "offload": {
[36m(Runner pid=1212922)[0m         "offload_params": true,
[36m(Runner pid=1212922)[0m         "offload_optimizer": true
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "global_batch_size_per_device": -1,
[36m(Runner pid=1212922)[0m       "disable_kl": false,
[36m(Runner pid=1212922)[0m       "use_kl_loss": true,
[36m(Runner pid=1212922)[0m       "kl_penalty": "low_var_kl",
[36m(Runner pid=1212922)[0m       "kl_coef": 0.01
[36m(Runner pid=1212922)[0m     },
[36m(Runner pid=1212922)[0m     "critic": {
[36m(Runner pid=1212922)[0m       "strategy": "fsdp",
[36m(Runner pid=1212922)[0m       "global_batch_size": 256,
[36m(Runner pid=1212922)[0m       "micro_batch_size_per_device_for_update": 4,
[36m(Runner pid=1212922)[0m       "micro_batch_size_per_device_for_experience": 16,
[36m(Runner pid=1212922)[0m       "max_grad_norm": 1.0,
[36m(Runner pid=1212922)[0m       "cliprange_value": 0.5,
[36m(Runner pid=1212922)[0m       "ppo_epochs": 1,
[36m(Runner pid=1212922)[0m       "padding_free": false,
[36m(Runner pid=1212922)[0m       "ulysses_sequence_parallel_size": 1,
[36m(Runner pid=1212922)[0m       "model": {
[36m(Runner pid=1212922)[0m         "model_path": null,
[36m(Runner pid=1212922)[0m         "tokenizer_path": null,
[36m(Runner pid=1212922)[0m         "override_config": {},
[36m(Runner pid=1212922)[0m         "enable_gradient_checkpointing": true,
[36m(Runner pid=1212922)[0m         "trust_remote_code": true,
[36m(Runner pid=1212922)[0m         "freeze_vision_tower": false
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "optim": {
[36m(Runner pid=1212922)[0m         "lr": 1e-06,
[36m(Runner pid=1212922)[0m         "betas": [
[36m(Runner pid=1212922)[0m           0.9,
[36m(Runner pid=1212922)[0m           0.999
[36m(Runner pid=1212922)[0m         ],
[36m(Runner pid=1212922)[0m         "weight_decay": 0.01,
[36m(Runner pid=1212922)[0m         "strategy": "adamw",
[36m(Runner pid=1212922)[0m         "lr_warmup_ratio": 0.0,
[36m(Runner pid=1212922)[0m         "min_lr_ratio": null,
[36m(Runner pid=1212922)[0m         "warmup_style": "constant",
[36m(Runner pid=1212922)[0m         "training_steps": -1
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "fsdp": {
[36m(Runner pid=1212922)[0m         "enable_full_shard": true,
[36m(Runner pid=1212922)[0m         "enable_cpu_offload": false,
[36m(Runner pid=1212922)[0m         "enable_rank0_init": false,
[36m(Runner pid=1212922)[0m         "use_orig_params": false,
[36m(Runner pid=1212922)[0m         "torch_dtype": null,
[36m(Runner pid=1212922)[0m         "fsdp_size": -1,
[36m(Runner pid=1212922)[0m         "mp_param_dtype": "bf16",
[36m(Runner pid=1212922)[0m         "mp_reduce_dtype": "fp32",
[36m(Runner pid=1212922)[0m         "mp_buffer_dtype": "fp32"
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "offload": {
[36m(Runner pid=1212922)[0m         "offload_params": false,
[36m(Runner pid=1212922)[0m         "offload_optimizer": false
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "global_batch_size_per_device": -1
[36m(Runner pid=1212922)[0m     },
[36m(Runner pid=1212922)[0m     "ref": {
[36m(Runner pid=1212922)[0m       "strategy": "fsdp",
[36m(Runner pid=1212922)[0m       "fsdp": {
[36m(Runner pid=1212922)[0m         "enable_full_shard": true,
[36m(Runner pid=1212922)[0m         "enable_cpu_offload": true,
[36m(Runner pid=1212922)[0m         "enable_rank0_init": true,
[36m(Runner pid=1212922)[0m         "use_orig_params": false,
[36m(Runner pid=1212922)[0m         "torch_dtype": "bf16",
[36m(Runner pid=1212922)[0m         "fsdp_size": -1,
[36m(Runner pid=1212922)[0m         "mp_param_dtype": "bf16",
[36m(Runner pid=1212922)[0m         "mp_reduce_dtype": "fp32",
[36m(Runner pid=1212922)[0m         "mp_buffer_dtype": "fp32"
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "offload": {
[36m(Runner pid=1212922)[0m         "offload_params": false,
[36m(Runner pid=1212922)[0m         "offload_optimizer": false
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "micro_batch_size_per_device_for_experience": 8,
[36m(Runner pid=1212922)[0m       "padding_free": true,
[36m(Runner pid=1212922)[0m       "ulysses_sequence_parallel_size": 1,
[36m(Runner pid=1212922)[0m       "use_torch_compile": true
[36m(Runner pid=1212922)[0m     },
[36m(Runner pid=1212922)[0m     "reward": {
[36m(Runner pid=1212922)[0m       "reward_type": "function",
[36m(Runner pid=1212922)[0m       "score_function": "/home/stud/wxie/EasyR1/examples/score_function/tool_reward.py",
[36m(Runner pid=1212922)[0m       "score_function_kwargs": {},
[36m(Runner pid=1212922)[0m       "skip_special_tokens": true,
[36m(Runner pid=1212922)[0m       "score_function_name": "compute_score"
[36m(Runner pid=1212922)[0m     },
[36m(Runner pid=1212922)[0m     "rollout": {
[36m(Runner pid=1212922)[0m       "name": "vllm",
[36m(Runner pid=1212922)[0m       "n": 6,
[36m(Runner pid=1212922)[0m       "temperature": 1.0,
[36m(Runner pid=1212922)[0m       "top_p": 0.99,
[36m(Runner pid=1212922)[0m       "top_k": -1,
[36m(Runner pid=1212922)[0m       "seed": 1,
[36m(Runner pid=1212922)[0m       "limit_images": 0,
[36m(Runner pid=1212922)[0m       "dtype": "bf16",
[36m(Runner pid=1212922)[0m       "gpu_memory_utilization": 0.6,
[36m(Runner pid=1212922)[0m       "ignore_eos": false,
[36m(Runner pid=1212922)[0m       "enforce_eager": false,
[36m(Runner pid=1212922)[0m       "enable_chunked_prefill": false,
[36m(Runner pid=1212922)[0m       "tensor_parallel_size": 1,
[36m(Runner pid=1212922)[0m       "max_model_len": null,
[36m(Runner pid=1212922)[0m       "max_num_batched_tokens": 8192,
[36m(Runner pid=1212922)[0m       "disable_log_stats": true,
[36m(Runner pid=1212922)[0m       "val_override_config": {
[36m(Runner pid=1212922)[0m         "temperature": 0.5,
[36m(Runner pid=1212922)[0m         "n": 1
[36m(Runner pid=1212922)[0m       },
[36m(Runner pid=1212922)[0m       "prompt_length": 2048,
[36m(Runner pid=1212922)[0m       "response_length": 2048,
[36m(Runner pid=1212922)[0m       "trust_remote_code": false
[36m(Runner pid=1212922)[0m     }
[36m(Runner pid=1212922)[0m   },
[36m(Runner pid=1212922)[0m   "algorithm": {
[36m(Runner pid=1212922)[0m     "gamma": 1.0,
[36m(Runner pid=1212922)[0m     "lam": 1.0,
[36m(Runner pid=1212922)[0m     "adv_estimator": "grpo",
[36m(Runner pid=1212922)[0m     "disable_kl": false,
[36m(Runner pid=1212922)[0m     "use_kl_loss": true,
[36m(Runner pid=1212922)[0m     "kl_penalty": "low_var_kl",
[36m(Runner pid=1212922)[0m     "kl_coef": 0.01,
[36m(Runner pid=1212922)[0m     "kl_type": "fixed",
[36m(Runner pid=1212922)[0m     "kl_horizon": 0.0,
[36m(Runner pid=1212922)[0m     "kl_target": 0.0
[36m(Runner pid=1212922)[0m   },
[36m(Runner pid=1212922)[0m   "trainer": {
[36m(Runner pid=1212922)[0m     "total_episodes": 1,
[36m(Runner pid=1212922)[0m     "max_steps": null,
[36m(Runner pid=1212922)[0m     "project_name": "Debug",
[36m(Runner pid=1212922)[0m     "experiment_name": "qwen2_5_vl_3b_grpo",
[36m(Runner pid=1212922)[0m     "logger": [
[36m(Runner pid=1212922)[0m       "console",
[36m(Runner pid=1212922)[0m       "wandb"
[36m(Runner pid=1212922)[0m     ],
[36m(Runner pid=1212922)[0m     "nnodes": 1,
[36m(Runner pid=1212922)[0m     "n_gpus_per_node": 1,
[36m(Runner pid=1212922)[0m     "critic_warmup": 0,
[36m(Runner pid=1212922)[0m     "val_freq": 5,
[36m(Runner pid=1212922)[0m     "val_before_train": true,
[36m(Runner pid=1212922)[0m     "val_only": false,
[36m(Runner pid=1212922)[0m     "val_generations_to_log": 3,
[36m(Runner pid=1212922)[0m     "save_freq": 5,
[36m(Runner pid=1212922)[0m     "save_limit": 3,
[36m(Runner pid=1212922)[0m     "save_checkpoint_path": "/home/stud/wxie/EasyR1/checkpoints/Debug/qwen2_5_vl_3b_grpo",
[36m(Runner pid=1212922)[0m     "load_checkpoint_path": null
[36m(Runner pid=1212922)[0m   }
[36m(Runner pid=1212922)[0m }
[36m(Runner pid=1212922)[0m Using score function `compute_score` from `/home/stud/wxie/EasyR1/examples/score_function/tool_reward.py`.
[36m(Runner pid=1212922)[0m Using score function `compute_score` from `/home/stud/wxie/EasyR1/examples/score_function/tool_reward.py`.
[36m(Runner pid=1212922)[0m loading dataset: BLINK-Benchmark/BLINK
[36m(Runner pid=1212922)[0m 
[36m(Runner pid=1212922)[0m loading dataset: BLINK-Benchmark/BLINK
[36m(Runner pid=1212922)[0m 
[36m(Runner pid=1212922)[0m Size of train dataloader: 7
[36m(Runner pid=1212922)[0m Size of val dataloader: 1
[36m(Runner pid=1212922)[0m the type of batch: <class 'dict'>
[36m(Runner pid=1212922)[0m the length of batch: 11
[36m(Runner pid=1212922)[0m key: dict_keys(['input_ids', 'attention_mask', 'position_ids', 'problem', 'idx', 'image_paths', 'message', 'multi_modal_data', 'multi_modal_inputs', 'raw_prompt_ids', 'ground_truth'])
[36m(Runner pid=1212922)[0m Tool usage reward: True
[36m(Runner pid=1212922)[0m Total training steps: 7
[36m(pid=1215812)[0m INFO 05-03 14:08:22 [__init__.py:239] Automatically detected platform cuda.
[36m(WorkerDict pid=1215812)[0m actor will use global batch size 48.
[36m(WorkerDict pid=1215812)[0m Model config: Qwen2_5_VLConfig {
[36m(WorkerDict pid=1215812)[0m   "architectures": [
[36m(WorkerDict pid=1215812)[0m     "Qwen2_5_VLForConditionalGeneration"
[36m(WorkerDict pid=1215812)[0m   ],
[36m(WorkerDict pid=1215812)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1215812)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=1215812)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1215812)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=1215812)[0m   "image_token_id": 151655,
[36m(WorkerDict pid=1215812)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1215812)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=1215812)[0m   "max_position_embeddings": 128000,
[36m(WorkerDict pid=1215812)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=1215812)[0m   "model_type": "qwen2_5_vl",
[36m(WorkerDict pid=1215812)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=1215812)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=1215812)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1215812)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1215812)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1215812)[0m   "rope_scaling": {
[36m(WorkerDict pid=1215812)[0m     "mrope_section": [
[36m(WorkerDict pid=1215812)[0m       16,
[36m(WorkerDict pid=1215812)[0m       24,
[36m(WorkerDict pid=1215812)[0m       24
[36m(WorkerDict pid=1215812)[0m     ],
[36m(WorkerDict pid=1215812)[0m     "rope_type": "default",
[36m(WorkerDict pid=1215812)[0m     "type": "default"
[36m(WorkerDict pid=1215812)[0m   },
[36m(WorkerDict pid=1215812)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=1215812)[0m   "sliding_window": 32768,
[36m(WorkerDict pid=1215812)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1215812)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1215812)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=1215812)[0m   "use_cache": true,
[36m(WorkerDict pid=1215812)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1215812)[0m   "video_token_id": 151656,
[36m(WorkerDict pid=1215812)[0m   "vision_config": {
[36m(WorkerDict pid=1215812)[0m     "depth": 32,
[36m(WorkerDict pid=1215812)[0m     "fullatt_block_indexes": [
[36m(WorkerDict pid=1215812)[0m       7,
[36m(WorkerDict pid=1215812)[0m       15,
[36m(WorkerDict pid=1215812)[0m       23,
[36m(WorkerDict pid=1215812)[0m       31
[36m(WorkerDict pid=1215812)[0m     ],
[36m(WorkerDict pid=1215812)[0m     "hidden_act": "silu",
[36m(WorkerDict pid=1215812)[0m     "hidden_size": 1280,
[36m(WorkerDict pid=1215812)[0m     "in_channels": 3,
[36m(WorkerDict pid=1215812)[0m     "in_chans": 3,
[36m(WorkerDict pid=1215812)[0m     "intermediate_size": 3420,
[36m(WorkerDict pid=1215812)[0m     "model_type": "qwen2_5_vl",
[36m(WorkerDict pid=1215812)[0m     "num_heads": 16,
[36m(WorkerDict pid=1215812)[0m     "out_hidden_size": 2048,
[36m(WorkerDict pid=1215812)[0m     "patch_size": 14,
[36m(WorkerDict pid=1215812)[0m     "spatial_merge_size": 2,
[36m(WorkerDict pid=1215812)[0m     "spatial_patch_size": 14,
[36m(WorkerDict pid=1215812)[0m     "temporal_patch_size": 2,
[36m(WorkerDict pid=1215812)[0m     "tokens_per_second": 2,
[36m(WorkerDict pid=1215812)[0m     "window_size": 112
[36m(WorkerDict pid=1215812)[0m   },
[36m(WorkerDict pid=1215812)[0m   "vision_end_token_id": 151653,
[36m(WorkerDict pid=1215812)[0m   "vision_start_token_id": 151652,
[36m(WorkerDict pid=1215812)[0m   "vision_token_id": 151654,
[36m(WorkerDict pid=1215812)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1215812)[0m }
[36m(WorkerDict pid=1215812)[0m 
[36m(WorkerDict pid=1215812)[0m Ulysses patch applied!
[36m(WorkerDict pid=1215812)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=1215812)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.35s/it]
[36m(WorkerDict pid=1215812)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.58s/it]
[36m(WorkerDict pid=1215812)[0m [rank0]:[W503 14:08:36.680778015 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[36m(WorkerDict pid=1215812)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=1215812)[0m Qwen2_5_VLForConditionalGeneration contains 3.75B parameters.
[36m(WorkerDict pid=1215812)[0m After huggingface model init: 2.27 GB / 93.02 GB.
[36m(WorkerDict pid=1215812)[0m FSDP wrap policy: functools.partial(<function transformer_auto_wrap_policy at 0x775bd21fed40>, transformer_layer_cls={<class 'transformers.models.qwen2_5_vl.modeling_qwen2_5_vl.Qwen2_5_VLVisionBlock'>, <class 'transformers.models.qwen2_5_vl.modeling_qwen2_5_vl.Qwen2_5_VLDecoderLayer'>}).
[36m(WorkerDict pid=1215812)[0m /home/wiss/liao/miniconda3/envs/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=1215812)[0m   warnings.warn(
